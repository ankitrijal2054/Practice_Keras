{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95e1ed43-b72e-42a0-98cc-c78f4d1dfb6a",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d533ca40-8918-439b-ade4-b852959ec7d6",
   "metadata": {},
   "source": [
    "# **Lab: Custom Training Loops in Keras**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb21332-2067-4dc1-9788-6809ed739a6a",
   "metadata": {},
   "source": [
    "Estimated time needed: **30** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e07912-e233-4249-ae1d-18029889caaf",
   "metadata": {},
   "source": [
    "In this lab, you will learn to implement a basic custom training loop in Keras. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc472da5-2ee1-457b-b623-5e7541df536b",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "By the end of this lab, you will: \n",
    "\n",
    "- Set up the environment \n",
    "\n",
    "- Define the neural network model \n",
    "\n",
    "- Define the Loss Function and Optimizer \n",
    "\n",
    "- Implement the custom training loop \n",
    "\n",
    "- Enhance the custom training loop by adding an accuracy metric to monitor model performance \n",
    "\n",
    "- Implement a custom callback to log additional metrics and information during training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcc4b26-ad4c-4fc5-99cf-c15b8df5dafb",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000a5343-2449-4858-b67b-b2848a1f1e3c",
   "metadata": {},
   "source": [
    "## Step-by-Step Instructions:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f05a3c6-920a-4368-a112-0fc707574e40",
   "metadata": {},
   "source": [
    "### Exercise 1: Basic custom training loop: \n",
    "\n",
    "#### 1. Set Up the Environment:\n",
    "\n",
    "- Import necessary libraries. \n",
    "\n",
    "- Load and preprocess the MNIST dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "728f25e0-fb12-49a0-a9cb-88312e48f5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import numpy as np\n",
    "\n",
    "# Suppress all Python warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set TensorFlow log level to suppress warnings and info messages\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Step 1: Set Up the Environment\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data() \n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0 \n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2233472b-ec3c-45ba-a190-8086c8f61a6a",
   "metadata": {},
   "source": [
    "#### 2. Define the model: \n",
    "\n",
    "Create a simple neural network model with a Flatten layer followed by two Dense layers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9ecb75e-7548-464d-bf40-d7bbff1a64e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define the Model\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39da66e3-1717-40ed-a8b5-413bb16dad08",
   "metadata": {},
   "source": [
    "#### 3. Define Loss Function and Optimizer: \n",
    "\n",
    "- Use Sparse Categorical Crossentropy for the loss function. \n",
    "- Use the Adam optimizer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb5839c9-2bc1-41bd-a0cd-8b1fe65f2c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define Loss Function and Optimizer\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) \n",
    "optimizer = tf.keras.optimizers.Adam()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c75bf9b-6e3e-4246-a718-51bc4122d618",
   "metadata": {},
   "source": [
    "#### 4. Implement the Custom Training Loop: \n",
    "\n",
    "- Iterate over the dataset for a specified number of epochs. \n",
    "- Compute the loss and apply gradients to update the model's weights. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80afaf2c-1e40-4146-8672-d1b59b32ed91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 1\n",
      "Epoch 1 Step 0: Loss = 2.3289215564727783\n",
      "Epoch 1 Step 200: Loss = 0.39350807666778564\n",
      "Epoch 1 Step 400: Loss = 0.2019486129283905\n",
      "Epoch 1 Step 600: Loss = 0.15402622520923615\n",
      "Epoch 1 Step 800: Loss = 0.1863749921321869\n",
      "Epoch 1 Step 1000: Loss = 0.3428850769996643\n",
      "Epoch 1 Step 1200: Loss = 0.1937139332294464\n",
      "Epoch 1 Step 1400: Loss = 0.21399268507957458\n",
      "Epoch 1 Step 1600: Loss = 0.21025016903877258\n",
      "Epoch 1 Step 1800: Loss = 0.18516576290130615\n",
      "Start of epoch 2\n",
      "Epoch 2 Step 0: Loss = 0.07841566205024719\n",
      "Epoch 2 Step 200: Loss = 0.14470304548740387\n",
      "Epoch 2 Step 400: Loss = 0.11146589368581772\n",
      "Epoch 2 Step 600: Loss = 0.04192083328962326\n",
      "Epoch 2 Step 800: Loss = 0.14068065583705902\n",
      "Epoch 2 Step 1000: Loss = 0.21892854571342468\n",
      "Epoch 2 Step 1200: Loss = 0.10258474946022034\n",
      "Epoch 2 Step 1400: Loss = 0.13193638622760773\n",
      "Epoch 2 Step 1600: Loss = 0.16499583423137665\n",
      "Epoch 2 Step 1800: Loss = 0.12055577337741852\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Implement the Custom Training Loop\n",
    "\n",
    "epochs = 2\n",
    "# train_dataset = train_dataset.repeat(epochs)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "for epoch in range(epochs):\n",
    "    print(f'Start of epoch {epoch + 1}')\n",
    "\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch_train, training=True)  # Forward pass\n",
    "            loss_value = loss_fn(y_batch_train, logits)  # Compute loss\n",
    "\n",
    "        # Compute gradients and update weights\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        # Logging the loss every 200 steps\n",
    "        if step % 200 == 0:\n",
    "            print(f'Epoch {epoch + 1} Step {step}: Loss = {loss_value.numpy()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238c0013-3e17-4937-a602-217111db3f22",
   "metadata": {},
   "source": [
    "### Exercise 2: Adding Accuracy Metric:\n",
    "\n",
    "Enhance the custom training loop by adding an accuracy metric to monitor model performance. \n",
    "\n",
    "#### 1. Set Up the Environment: \n",
    "\n",
    "Follow the setup from Exercise 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b891a118-9ed8-4806-929c-baf71a646d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Flatten \n",
    "\n",
    "# Step 1: Set Up the Environment\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the pixel values to be between 0 and 1\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0 \n",
    "\n",
    "# Create a batched dataset for training\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cfea84-f1df-407d-9f84-0adb91e3bba7",
   "metadata": {},
   "source": [
    "#### 2. Define the Model: \n",
    "Use the same model as in Exercise 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7263943a-fcaf-4b16-b24c-25f80911fe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define the Model\n",
    "\n",
    "model = Sequential([ \n",
    "    Flatten(input_shape=(28, 28)),  # Flatten the input to a 1D vector\n",
    "    Dense(128, activation='relu'),  # First hidden layer with 128 neurons and ReLU activation\n",
    "    Dense(10)  # Output layer with 10 neurons for the 10 classes (digits 0-9)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bacfd4-5dbb-4f93-a388-bfdd1da07c59",
   "metadata": {},
   "source": [
    "#### 3. Define the loss function, optimizer, and metric: \n",
    "\n",
    "- Use Sparse Categorical Crossentropy for the loss function and Adam optimizer. \n",
    "\n",
    "- Add Sparse Categorical Accuracy as a metric. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b425d918-3140-4e52-9e61-14e7b192ae7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define Loss Function, Optimizer, and Metric\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Loss function for multi-class classification\n",
    "optimizer = tf.keras.optimizers.Adam()  # Adam optimizer for efficient training\n",
    "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy()  # Metric to track accuracy during training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba0bb46-7671-4192-b624-8b40e3bc8a00",
   "metadata": {},
   "source": [
    "#### 4. Implement the custom training loop with accuracy: \n",
    "\n",
    "Track the accuracy during training and print it at regular intervals. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bb3041b-89b1-41f5-a557-4b9fad10236a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 1\n",
      "Epoch 1 Step 0: Loss = 2.4252877235412598 Accuracy = 0.09375\n",
      "Epoch 1 Step 200: Loss = 0.3713875412940979 Accuracy = 0.8381530046463013\n",
      "Epoch 1 Step 400: Loss = 0.1912544071674347 Accuracy = 0.8697786927223206\n",
      "Epoch 1 Step 600: Loss = 0.19827668368816376 Accuracy = 0.8851913213729858\n",
      "Epoch 1 Step 800: Loss = 0.17002084851264954 Accuracy = 0.8970427513122559\n",
      "Epoch 1 Step 1000: Loss = 0.44687652587890625 Accuracy = 0.9040958881378174\n",
      "Epoch 1 Step 1200: Loss = 0.16038818657398224 Accuracy = 0.9103611707687378\n",
      "Epoch 1 Step 1400: Loss = 0.2522040009498596 Accuracy = 0.9149714708328247\n",
      "Epoch 1 Step 1600: Loss = 0.20981311798095703 Accuracy = 0.9177271723747253\n",
      "Epoch 1 Step 1800: Loss = 0.20408540964126587 Accuracy = 0.9215192794799805\n",
      "Start of epoch 2\n",
      "Epoch 2 Step 0: Loss = 0.08710051327943802 Accuracy = 0.96875\n",
      "Epoch 2 Step 200: Loss = 0.14900946617126465 Accuracy = 0.9586442708969116\n",
      "Epoch 2 Step 400: Loss = 0.11503057181835175 Accuracy = 0.9562811851501465\n",
      "Epoch 2 Step 600: Loss = 0.053975660353899 Accuracy = 0.9579867124557495\n",
      "Epoch 2 Step 800: Loss = 0.09721027314662933 Accuracy = 0.9595427513122559\n",
      "Epoch 2 Step 1000: Loss = 0.2825801372528076 Accuracy = 0.960102379322052\n",
      "Epoch 2 Step 1200: Loss = 0.10017538070678711 Accuracy = 0.9611521363258362\n",
      "Epoch 2 Step 1400: Loss = 0.1373341828584671 Accuracy = 0.962192177772522\n",
      "Epoch 2 Step 1600: Loss = 0.11700668185949326 Accuracy = 0.9621134996414185\n",
      "Epoch 2 Step 1800: Loss = 0.09374967962503433 Accuracy = 0.9630587100982666\n",
      "Start of epoch 3\n",
      "Epoch 3 Step 0: Loss = 0.03582335263490677 Accuracy = 1.0\n",
      "Epoch 3 Step 200: Loss = 0.1061677634716034 Accuracy = 0.9740360975265503\n",
      "Epoch 3 Step 400: Loss = 0.08133839070796967 Accuracy = 0.9721009731292725\n",
      "Epoch 3 Step 600: Loss = 0.027977092191576958 Accuracy = 0.9733257293701172\n",
      "Epoch 3 Step 800: Loss = 0.04544239863753319 Accuracy = 0.9741728901863098\n",
      "Epoch 3 Step 1000: Loss = 0.21526162326335907 Accuracy = 0.9741820693016052\n",
      "Epoch 3 Step 1200: Loss = 0.06337268650531769 Accuracy = 0.9745784997940063\n",
      "Epoch 3 Step 1400: Loss = 0.08487281948328018 Accuracy = 0.9751070737838745\n",
      "Epoch 3 Step 1600: Loss = 0.08048281818628311 Accuracy = 0.9750156402587891\n",
      "Epoch 3 Step 1800: Loss = 0.05647951364517212 Accuracy = 0.9757079482078552\n",
      "Start of epoch 4\n",
      "Epoch 4 Step 0: Loss = 0.02241399511694908 Accuracy = 1.0\n",
      "Epoch 4 Step 200: Loss = 0.07712739706039429 Accuracy = 0.9836753606796265\n",
      "Epoch 4 Step 400: Loss = 0.07314810901880264 Accuracy = 0.9818422794342041\n",
      "Epoch 4 Step 600: Loss = 0.021334253251552582 Accuracy = 0.982737123966217\n",
      "Epoch 4 Step 800: Loss = 0.05113351345062256 Accuracy = 0.9830290079116821\n",
      "Epoch 4 Step 1000: Loss = 0.17704415321350098 Accuracy = 0.9830482006072998\n",
      "Epoch 4 Step 1200: Loss = 0.03247726708650589 Accuracy = 0.9826446771621704\n",
      "Epoch 4 Step 1400: Loss = 0.038788046687841415 Accuracy = 0.9826909303665161\n",
      "Epoch 4 Step 1600: Loss = 0.0586332231760025 Accuracy = 0.9825499653816223\n",
      "Epoch 4 Step 1800: Loss = 0.04769159108400345 Accuracy = 0.982822060585022\n",
      "Start of epoch 5\n",
      "Epoch 5 Step 0: Loss = 0.015521585941314697 Accuracy = 1.0\n",
      "Epoch 5 Step 200: Loss = 0.06096682697534561 Accuracy = 0.9884950518608093\n",
      "Epoch 5 Step 400: Loss = 0.07836732268333435 Accuracy = 0.9870635867118835\n",
      "Epoch 5 Step 600: Loss = 0.018249977380037308 Accuracy = 0.9875208139419556\n",
      "Epoch 5 Step 800: Loss = 0.04698978736996651 Accuracy = 0.9874765872955322\n",
      "Epoch 5 Step 1000: Loss = 0.15040823817253113 Accuracy = 0.9873563647270203\n",
      "Epoch 5 Step 1200: Loss = 0.03422006592154503 Accuracy = 0.9871461391448975\n",
      "Epoch 5 Step 1400: Loss = 0.015723807737231255 Accuracy = 0.9871966242790222\n",
      "Epoch 5 Step 1600: Loss = 0.04369444400072098 Accuracy = 0.9871760010719299\n",
      "Epoch 5 Step 1800: Loss = 0.042231567203998566 Accuracy = 0.9871945977210999\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Implement the Custom Training Loop with Accuracy\n",
    "\n",
    "epochs = 5  # Number of epochs for training\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Start of epoch {epoch + 1}')\n",
    "    \n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass: Compute predictions\n",
    "            logits = model(x_batch_train, training=True)\n",
    "            # Compute loss\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "        \n",
    "        # Compute gradients\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        # Apply gradients to update model weights\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        \n",
    "        # Update the accuracy metric\n",
    "        accuracy_metric.update_state(y_batch_train, logits)\n",
    "\n",
    "        # Log the loss and accuracy every 200 steps\n",
    "        if step % 200 == 0:\n",
    "            print(f'Epoch {epoch + 1} Step {step}: Loss = {loss_value.numpy()} Accuracy = {accuracy_metric.result().numpy()}')\n",
    "    \n",
    "    # Reset the metric at the end of each epoch\n",
    "    accuracy_metric.reset_state()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aad4044-971c-4c8d-bd28-d8fc21cdaba5",
   "metadata": {},
   "source": [
    "### Exercise 3: Custom Callback for Advanced Logging: \n",
    "\n",
    "Implement a custom callback to log additional metrics and information during training. \n",
    "\n",
    "#### 1. Set Up the Environment: \n",
    "\n",
    "Follow the setup from Exercise 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dc7593c-59fe-4d2b-ae24-04277b43ba83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Flatten \n",
    "\n",
    "# Step 1: Set Up the Environment\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the pixel values to be between 0 and 1\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0 \n",
    "\n",
    "# Create a batched dataset for training\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e17f468-52d3-4f74-9160-49244766c548",
   "metadata": {},
   "source": [
    "#### 2. Define the Model: \n",
    "\n",
    "Use the same model as in Exercise 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7322571-c9f4-4ecb-b57a-a2051660692f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define the Model\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),  # Flatten the input to a 1D vector\n",
    "    Dense(128, activation='relu'),  # First hidden layer with 128 neurons and ReLU activation\n",
    "    Dense(10)  # Output layer with 10 neurons for the 10 classes (digits 0-9)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f674612-bef3-4945-b39c-c11e7685ac5b",
   "metadata": {},
   "source": [
    "#### 3. Define Loss Function, Optimizer, and Metric: \n",
    "\n",
    "- Use Sparse Categorical Crossentropy for the loss function and Adam optimizer. \n",
    "\n",
    "- Add Sparse Categorical Accuracy as a metric. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c07e5957-4fca-45ff-aca2-597a30303f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define Loss Function, Optimizer, and Metric\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Loss function for multi-class classification\n",
    "optimizer = tf.keras.optimizers.Adam()  # Adam optimizer for efficient training\n",
    "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy()  # Metric to track accuracy during training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6cf7ff-9484-4bca-a1fe-3dc5641bfdd7",
   "metadata": {},
   "source": [
    "#### 4. Implement the custom training loop with custom callback: \n",
    "\n",
    "Create a custom callback to log additional metrics at the end of each epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab211a00-900c-4d4a-bbec-4b189d4152d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "# Step 4: Implement the Custom Callback \n",
    "class CustomCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        print(f'End of epoch {epoch + 1}, loss: {logs.get(\"loss\")}, accuracy: {logs.get(\"accuracy\")}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96faf68c-e238-4c03-a37c-0a05e90e15c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 1\n",
      "Epoch 1 Step 0: Loss = 2.409320831298828 Accuracy = 0.1875\n",
      "Epoch 1 Step 200: Loss = 0.38837480545043945 Accuracy = 0.8376865386962891\n",
      "Epoch 1 Step 400: Loss = 0.18251235783100128 Accuracy = 0.8703241944313049\n",
      "Epoch 1 Step 600: Loss = 0.18625980615615845 Accuracy = 0.8851393461227417\n",
      "Epoch 1 Step 800: Loss = 0.16050311923027039 Accuracy = 0.8971207737922668\n",
      "Epoch 1 Step 1000: Loss = 0.4694017171859741 Accuracy = 0.9043144583702087\n",
      "Epoch 1 Step 1200: Loss = 0.1899411380290985 Accuracy = 0.9104132056236267\n",
      "Epoch 1 Step 1400: Loss = 0.26929065585136414 Accuracy = 0.9152390956878662\n",
      "Epoch 1 Step 1600: Loss = 0.22808891534805298 Accuracy = 0.917980968952179\n",
      "Epoch 1 Step 1800: Loss = 0.14185668528079987 Accuracy = 0.9218142628669739\n",
      "End of epoch 1, loss: 0.04081527888774872, accuracy: 0.9238499999046326\n",
      "Start of epoch 2\n",
      "Epoch 2 Step 0: Loss = 0.08502554893493652 Accuracy = 1.0\n",
      "Epoch 2 Step 200: Loss = 0.15162897109985352 Accuracy = 0.9603545069694519\n",
      "Epoch 2 Step 400: Loss = 0.11262986809015274 Accuracy = 0.9574501514434814\n",
      "Epoch 2 Step 600: Loss = 0.04105581343173981 Accuracy = 0.960066556930542\n",
      "Epoch 2 Step 800: Loss = 0.10088969767093658 Accuracy = 0.9611423015594482\n",
      "Epoch 2 Step 1000: Loss = 0.359790563583374 Accuracy = 0.9611014127731323\n",
      "Epoch 2 Step 1200: Loss = 0.1080564558506012 Accuracy = 0.9621149301528931\n",
      "Epoch 2 Step 1400: Loss = 0.15973429381847382 Accuracy = 0.9630398154258728\n",
      "Epoch 2 Step 1600: Loss = 0.15071551501750946 Accuracy = 0.9629723429679871\n",
      "Epoch 2 Step 1800: Loss = 0.08235988765954971 Accuracy = 0.9637354016304016\n",
      "End of epoch 2, loss: 0.022920232266187668, accuracy: 0.9644666910171509\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Implement the Custom Training Loop with Custom Callback\n",
    "\n",
    "epochs = 2\n",
    "custom_callback = CustomCallback()  # Initialize the custom callback\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Start of epoch {epoch + 1}')\n",
    "    \n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass: Compute predictions\n",
    "            logits = model(x_batch_train, training=True)\n",
    "            # Compute loss\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "        \n",
    "        # Compute gradients\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        # Apply gradients to update model weights\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        \n",
    "        # Update the accuracy metric\n",
    "        accuracy_metric.update_state(y_batch_train, logits)\n",
    "\n",
    "        # Log the loss and accuracy every 200 steps\n",
    "        if step % 200 == 0:\n",
    "            print(f'Epoch {epoch + 1} Step {step}: Loss = {loss_value.numpy()} Accuracy = {accuracy_metric.result().numpy()}')\n",
    "    \n",
    "    # Call the custom callback at the end of each epoch\n",
    "    custom_callback.on_epoch_end(epoch, logs={'loss': loss_value.numpy(), 'accuracy': accuracy_metric.result().numpy()})\n",
    "    \n",
    "    # Reset the metric at the end of each epoch\n",
    "    accuracy_metric.reset_state()  # Use reset_state() instead of reset_states()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaa30a2-3560-4d90-b4d2-53346de3bc48",
   "metadata": {},
   "source": [
    "### Exercise 4: Lab - Hyperparameter Tuning \n",
    "\n",
    "#### Enhancement: Add functionality to save the results of each hyperparameter tuning iteration as JSON files in a specified directory. \n",
    "\n",
    "#### Additional Instructions:\n",
    "\n",
    "Modify the tuning loop to save each iteration's results as JSON files.\n",
    "\n",
    "Specify the directory where these JSON files will be stored for easier retrieval and analysis of tuning results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eb3640-4655-4c5e-8b91-e577e68b8b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 00m 01s]\n",
      "val_accuracy: 0.9449999928474426\n",
      "\n",
      "Best val_accuracy So Far: 0.9800000190734863\n",
      "Total elapsed time: 00h 00m 08s\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'tuning_results/trial_1.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 53\u001b[0m\n\u001b[1;32m     46\u001b[0m         results \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     47\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrial\u001b[39m\u001b[38;5;124m\"\u001b[39m: i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     48\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhyperparameters\u001b[39m\u001b[38;5;124m\"\u001b[39m: best_hps\u001b[38;5;241m.\u001b[39mvalues,  \u001b[38;5;66;03m# Hyperparameters tuned in this trial\u001b[39;00m\n\u001b[1;32m     49\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# Add any score or metrics if available\u001b[39;00m\n\u001b[1;32m     50\u001b[0m         }\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;66;03m# Save the results as JSON\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtuning_results\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrial_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     54\u001b[0m             json\u001b[38;5;241m.\u001b[39mdump(results, f)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'tuning_results/trial_1.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Step 1: Load your dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Step 2: Define the model-building function\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    # Tune the number of units in the first Dense layer\n",
    "    model.add(Dense(units=hp.Int('units', min_value=32, max_value=512, step=32),\n",
    "                    activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Binary classification example\n",
    "    model.compile(optimizer=Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling='LOG')),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Step 3: Initialize a Keras Tuner RandomSearch tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,  # Set the number of trials\n",
    "    executions_per_trial=1,  # Set how many executions per trial\n",
    "    directory='tuner_results',  # Directory for saving logs\n",
    "    project_name='hyperparam_tuning'\n",
    ")\n",
    "\n",
    "# Step 4: Run the tuner search (make sure the data is correct)\n",
    "tuner.search(X_train, y_train, validation_data=(X_val, y_val), epochs=5)\n",
    "\n",
    "# Step 5: Save the tuning results as JSON files\n",
    "try:\n",
    "    for i in range(10):\n",
    "        # Fetch the best hyperparameters from the tuner\n",
    "        best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "        \n",
    "        # Results dictionary to save hyperparameters and score\n",
    "        results = {\n",
    "            \"trial\": i + 1,\n",
    "            \"hyperparameters\": best_hps.values,  # Hyperparameters tuned in this trial\n",
    "            \"score\": None  # Add any score or metrics if available\n",
    "        }\n",
    "\n",
    "        # Save the results as JSON\n",
    "        with open(os.path.join('tuning_results', f\"trial_{i + 1}.json\"), \"w\") as f:\n",
    "            json.dump(results, f)\n",
    "\n",
    "except IndexError:\n",
    "    print(\"Tuning process has not completed or no results available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e579e35-aa44-4d80-be3f-f72112a76ab4",
   "metadata": {},
   "source": [
    "### Conclusion: \n",
    "\n",
    "Congratulations on completing this lab! You have now successfully created, trained, and evaluated a simple neural network model using the Keras Functional API. This foundational knowledge will allow you to build more complex models and explore advanced functionalities in Keras. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5f8b86-5f72-4904-aa74-b8323e22a484",
   "metadata": {},
   "source": [
    "Copyright © IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "prev_pub_hash": "48a1eb2565c8b635156cd21708473ccadb84e292e93f3530a9d5223b7590344e"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
